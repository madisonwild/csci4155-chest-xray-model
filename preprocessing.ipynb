{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd6bb830",
   "metadata": {},
   "source": [
    "# ChestX-ray 14 Data Preprocessing\n",
    "## Madison Moffat-Wild and Rachel Woodside\n",
    "\n",
    "### https://paperswithcode.com/dataset/chestx-ray14\n",
    "### https://www.kaggle.com/datasets/nih-chest-xrays/data/data\n",
    "\n",
    "Resources:\n",
    "https://machinelearningmastery.com/best-practices-for-preparing-and-augmenting-image-data-for-convolutional-neural-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319c41e8",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f52e1ce",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae3045da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59beffc",
   "metadata": {},
   "source": [
    "### Initialize Path to Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be105ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store path to data folder\n",
    "#data_folder = \"~/ChestXray14Data/\"\n",
    "data_folder = \"data/\"\n",
    "data_dir = pathlib.Path(data_folder).with_suffix('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6a577b",
   "metadata": {},
   "source": [
    "### Read in Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bb15c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Read in bbox?\n",
    "dataEntry = pd.read_csv(data_folder+'Data_Entry_2017.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ff38ed",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b409a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fix column names\n",
    "dataEntry = dataEntry.rename(columns={\n",
    "    \"OriginalImagePixelSpacing[x\": \"OriginalPixelSpacingX\",\n",
    "    \"y]\": \"OriginalPixelSpacingY\",\n",
    "    \"OriginalImage[Width\": \"OriginalImageWidth\",\n",
    "    \"Height]\": \"OriginalImageHeight\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69428aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fixing nonsensical values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af25c413",
   "metadata": {},
   "source": [
    "## Data Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55b87d2",
   "metadata": {},
   "source": [
    "### Load in Image Data\n",
    "Reference: https://www.tensorflow.org/tutorials/load_data/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4d6ef524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112120\n"
     ]
    }
   ],
   "source": [
    "image_count = len(list(data_dir.glob('*/*/*.png')))\n",
    "print(image_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ebbafba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'data\\\\images_006\\\\images\\\\00012834_073.png'\n",
      "b'data\\\\images_010\\\\images\\\\00022352_002.png'\n",
      "b'data\\\\images_006\\\\images\\\\00012622_023.png'\n",
      "b'data\\\\images_006\\\\images\\\\00012797_000.png'\n",
      "b'data\\\\images_008\\\\images\\\\00017430_000.png'\n"
     ]
    }
   ],
   "source": [
    "list_ds = tf.data.Dataset.list_files(str(data_dir/'*/*/*'), shuffle=False)\n",
    "list_ds = list_ds.shuffle(image_count, reshuffle_each_iteration=False)\n",
    "for f in list_ds.take(5):\n",
    "    print(f.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6b559ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = int(image_count * 0.2)\n",
    "train_ds = list_ds.skip(val_size)\n",
    "val_ds = list_ds.take(val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f891d99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89696\n",
      "22424\n"
     ]
    }
   ],
   "source": [
    "print(tf.data.experimental.cardinality(train_ds).numpy())\n",
    "print(tf.data.experimental.cardinality(val_ds).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e939a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: modify to retrieve the labels from the dataEntry dataframe\n",
    "def get_label(file_path):\n",
    "    # Convert the path to a list of path components\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    # The last is the file's name?\n",
    "    one_hot = parts[-1]\n",
    "    # TODO: Strip .png?\n",
    "    # TODO: Decide what to return... probably a vector representing the one-hot encoding?\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2445c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_img(img):\n",
    "    # channels=1 will give a grayscale image\n",
    "    img = tf.io.decode_png(img, channels=1)\n",
    "    # Resize the image to the desired size\n",
    "    img_height = 256\n",
    "    img_width = 256\n",
    "    return tf.image.resize(img, [img_height, img_width])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fe8baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_path(file_path):\n",
    "    label = get_label(file_path)\n",
    "    # Load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d81f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: convert images to arrays of pixel values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932ed679",
   "metadata": {},
   "source": [
    "## Data Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd25743",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0614c41",
   "metadata": {},
   "source": [
    "### Generate One-Hot Encodings for Output Labels\n",
    "Reference: https://www.kaggle.com/code/ashishmundu/nih-chest-x-rays-deep-convolutional-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4bf6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Normalize pixel values?\n",
    "# TODO: Reshape images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8392e82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Emphysema</th>\n",
       "      <th>Effusion</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Hernia</th>\n",
       "      <th>Infiltration</th>\n",
       "      <th>Mass</th>\n",
       "      <th>Nodule</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Pleural_Thickening</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Fibrosis</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Consolidation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cardiomegaly  Emphysema  Effusion  No Finding  Hernia  Infiltration  Mass  \\\n",
       "0           1.0        0.0       0.0         0.0     0.0           0.0   0.0   \n",
       "1           1.0        1.0       0.0         0.0     0.0           0.0   0.0   \n",
       "2           1.0        0.0       1.0         0.0     0.0           0.0   0.0   \n",
       "3           0.0        0.0       0.0         1.0     0.0           0.0   0.0   \n",
       "4           0.0        0.0       0.0         0.0     1.0           0.0   0.0   \n",
       "\n",
       "   Nodule  Atelectasis  Pneumothorax  Pleural_Thickening  Pneumonia  Fibrosis  \\\n",
       "0     0.0          0.0           0.0                 0.0        0.0       0.0   \n",
       "1     0.0          0.0           0.0                 0.0        0.0       0.0   \n",
       "2     0.0          0.0           0.0                 0.0        0.0       0.0   \n",
       "3     0.0          0.0           0.0                 0.0        0.0       0.0   \n",
       "4     0.0          0.0           0.0                 0.0        0.0       0.0   \n",
       "\n",
       "   Edema  Consolidation  \n",
       "0    0.0            0.0  \n",
       "1    0.0            0.0  \n",
       "2    0.0            0.0  \n",
       "3    0.0            0.0  \n",
       "4    0.0            0.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Generate one-hot encoding for the labels\n",
    "\n",
    "# Get all the label\n",
    "uniqueLabels = pd.Series(itertools.chain.from_iterable(dataEntry[\"Finding Labels\"].apply(lambda x : x.split('|')))).unique()\n",
    "\n",
    "# Initialize an empty matrix \n",
    "oneHotEncodings = pd.DataFrame(0.0, index=np.arange(len(dataEntry)), columns=uniqueLabels)\n",
    "\n",
    "# For each row, we get the associated labels and set a 1 to the new corresponding column label \n",
    "for index, row in dataEntry.iterrows():\n",
    "    labels = row[\"Finding Labels\"].split('|')\n",
    "    for label in labels:\n",
    "        oneHotEncodings.iloc[index][label] = 1.0\n",
    "\n",
    "# Then, we concatenate this new dataframe to our original data\n",
    "#dataEntry = pd.concat([dataEntry, oneHotEncodings], axis=1)\n",
    "\n",
    "oneHotEncodings.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
